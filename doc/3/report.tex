\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage[a4paper]{geometry}
\usepackage{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 3 - Text Classification Report}
\author{anufer@student.ethz.ch\\ elmerl@student.ethz.ch\\ nivo@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}
Usage:\\
Download the csv files to /data/3/....csv (... = training, testing, validation)\\
Run data/fix\_csvs.rb (Ruby), compile and run C\# project, run code/map.m (Matlab) \\
Results are in /data/3/....out (... = training, testing\_map, validation\_map).





\section{Tools}
We used .NET to do the pre-processing (Create word bag, assign entries to word list, fix input data, etc) and Mathlab to do the classification.

\begin{itemize}
\item Ruby (code is in /data/3)
\item Visual Studio (C\#, LINQ) (code is in /code/3\_map/preprocessing/external\_tools/ML-3 directory)
\item Matlab (code is in /code/ directory)
\item Git / \href{https://github.com/lukaselmer/ethz-machine-learning}{Github Repository} \footnote{https://github.com/lukaselmer/ethz-machine-learning}
  %\begin{itemize}
    % item \href{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3}{Final solution} \footnote{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3} (tag best\_v3, master branch, code attached in the zip file) 
  %\end{itemize}
\end{itemize}

\section{Algorithm}

We used
\begin{itemize}
\item Some sort of edit distance (not the Levinsthein distance however)
\item Maximum a posteriori (MAP) estimation
\end{itemize}

To classify the data we used the maximum a posteriori probability (MAP) estimate method presented in the tutorial session in week 49. We implemented the algorithm for mulinomial distribution for a flexible number of k. The MAP is implemented in Matlab.


\section{Features}
First, we used pre-pre-processing to clean up the invalid CSV files. For this, we used Ruby\footnote{https://www.ruby-lang.org/}.

In our implementation, the pre-processing step consists of the preparation of the input data, the selection of the words to put in the \textit{bag of words} of the Maximum a posteriori (MAP) estimation and the mapping of the input data to a mapping to the selected words. 

To prepare the data, we got a list of the top 400 words in the entire training set and we replaced every other word that is similar enough to one of this top word. The similarity is calculated by the edit distance of two words divided by the length of the longer of the two words. Two words are considered similar if the similarity is greater than 0.75.

To group similar words together, preprocessing was used. First, we used the Levinsthein distance\footnote{https://en.wikipedia.org/wiki/Levenshtein\_distance}, but then switched to a slightly modified version of the edit distance\footnote{https://en.wikipedia.org/wiki/Edit\_distance}, which gave slightly better results. Also, instead of using an absolute value threshold for the distance, we used a ratio of about 75\% to group similar words together. For this preprocessing, we used C\#, because string handling in C\# seemed much easier than in Matlab.

Additionally, stemming was implemented in a very simple way: if the word is longer than 10 characters, the last three characters are ignored for the edit distance.

To fill the bag of words we group the training data by classifier and count the occurrence of each word. We take the top 25 words for each classifier if they appear in at least 10\% of the training entries of this classifier.

To be usable for the MAP algorithm, each data entry is then mapped to a vector $v$ with $v_i \in \lbrace0, 1\rbrace$. $v_i$ = 1 indicates that a certain word $i$ from the bag occurred in the data entry and $v_i$ = 0 it doesn't.

\section{Parameters}
To find the parameters, we used manual testing. The most important feature seemed to be the edit distance ratio, which is 75\%. Another important parameter is the amount of top words which are picked at the start of the algorithm. These are the words, which are in a category of it's own before grouping them together.

The MAP algorithm has one parameter $\alpha$: $\alpha_i \in \lbrace1, k \rbrace = 1.01$.

\section{Lessons Learned}
Many other tools and algorithms have been tried. However, one we didn't try and might have worked well is SVM.

What we also learned is that string handling is painful in Matlab, especially when compared to the C\#'s LINQ queries. It therefore may be a good idea to use different technologies for a project, so each technology's strengths can be used.

\end{document} 
