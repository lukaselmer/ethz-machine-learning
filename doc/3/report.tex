\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage[a4paper]{geometry}
\usepackage{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 3 - Text Classification Report}
\author{anufer@student.ethz.ch\\ elmerl@student.ethz.ch\\ nivo@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}
Usage:\\
Download the csv files to /data/3/....csv (... = training, testing, validation)\\
Run map.m \\
Results are in /data/3/....out (... = training, testing, validation)










\section{Tools}
We used .NET to do the pre-processing (Create word bag, assign entries to word list, fix input data, etc) and Mathlab to do the classification.

\begin{itemize}
\item Visual Studio (C\#, LINQ) (code is in /code/3\_map/preprocessing/external\_tools/ML-3 directory)
\item Matlab (code is in /code/ directory)
\item Git / \href{https://github.com/lukaselmer/ethz-machine-learning}{Github Repository} \footnote{https://github.com/lukaselmer/ethz-machine-learning}
  %\begin{itemize}
    % item \href{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3}{Final solution} \footnote{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3} (tag best\_v3, master branch, code attached in the zip file) 
  %\end{itemize}
\end{itemize}

\section{Algorithm}

Principal component analysis

Describe the algorithm you used for classification.

\section{Features}
To group similar words together, reprocessing was used. First, we used the Levinsthein distance\footnote{https://en.wikipedia.org/wiki/Levenshtein\_distance}, but then switched to a slightly modified version of the edit distance\footnote{https://en.wikipedia.org/wiki/Edit\_distance}, which gave slightly better results. Also, instead of using an absolute value threshold for the distance, we used a ratio of about 75\% to group similar words together. For this preprocessing, we used C\#, because string handling in C\# seemed much easier than in Matlab.\\

After the preprocessing, we then used Matlab to predict the city codes and country codes. For this, we used PCA.

In our implementation, the pre-processing step consists of the preparation of the input data, the selection of the words to put in the \textit{bag of words} of the MAP and the mapping of the input data to a mapping to the selected words. 

To prepare the data, we got a list of the top 400 words in the entire training set and we replaced every other word that is similar enough to one of this top word. The similarity is calculated by the edit distance of two words divided by the length of the longer of the two words. Two words are considered similar if the similarity is greater than 0.75.

To fill the bag of words we group the training data by classifier and count the occurrence of each word. We take the top 25 words for each classifier if they appear in at least 10\% of the training entries of this classifier.

To be usable for the MAP algorithm, each data entry is then mapped to a vector $v$ with $v_i \in \lbrace0, 1\rbrace$. $v_i$ = 1 indicates that a certain word $i$ from the bag occurred in the data entry and $v_i$ = 0 it doesn't.

\section{Parameters}
To find the parameters, we used manual testing. The most important feature seemed to be the edit distance ratio, which is 75\%. Another important parameter is the amount of top words which are picked at the start of the algorithm. These are the words, which are in a category of it's own before grouping them together.

\section{Lessons Learned}
Many other tools and algorithms have been tried. However, one we didn't try and might have worked well is SVM.

\end{document} 
