\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 3 - Text Classification Report}
\author{anufer@student.ethz.ch\\ elmerl@student.ethz.ch\\ nivo@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}
Usage:\\
Download the csv files to /data/3/....csv (... = training, testing, validation)\\
Run map.m \\
Results are in /data/3/....out (... = training, testing, validation)

\section{Tools}
We used .NET to do the pre-processing (Create word bag, assign entries to word list, fix input data, etc) and Mathlab to do the classification.

\begin{itemize}
\item Visual Studio (C\#) (code is in /code/3\_map/preprocessing/external\_tools/ML-3 directory)
\item Matlab (code is in /code/ directory)
\item Git / \href{https://github.com/lukaselmer/ethz-machine-learning}{Github Repository} \footnote{https://github.com/lukaselmer/ethz-machine-learning}
  %\begin{itemize}
    % item \href{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3}{Final solution} \footnote{https://github.com/lukaselmer/ethz-machine-learning/releases/tag/best\_v3} (tag best\_v3, master branch, code attached in the zip file) 
  %\end{itemize}
\end{itemize}

\section{Algorithm}
To classify the data we used the maximum a posteriori probability (MAP) estimate method  presented in the tutorial session in week 49. We implemented the algorithm for mulinomial distribution for a flexible number of k.
The MAP is implemented in Matlab.

\section{Features}
In our implementation, the pre-processing step consists of the preparation of the input data, the selection of the words to put in the \textit{bag of words} of the MAP and the mapping of the input data to a mapping to the selected words. 

To prepare the data, we got a list of the top 400 words in the entire training set and we replaced every other word that is similar enough to one of this top word. The similarity is calculated by the edit distance of two words divided by the length of the longer of the two words. Two words are considered similar if the similarity is greater than 0.75.

To fill the bag of words we group the training data by classifier and count the occurrence of each word. We take the top 25 words for each classifier if they appear in at least 10\% of the training entries of this classifier.

To be usable for the MAP algorithm, each data entry is then mapped to a vector $v$ with $v_i \in \lbrace0, 1\rbrace$. $v_i$ = 1 indicates that a certain word $i$ from the bag occurred in the data entry and $v_i$ = 0 it doesn't.

The pre-processing was implemented in C\#.

\section{Parameters}
The MAP algorithm has one parameter $\alpha$: $\alpha_i \in \lbrace1, k \rbrace = 1.01$



\section{Lessons Learned} What other algorithms did you try out that didn't work well?
Why do you think they performed worse than what you used for your final submission?

\end{document} 
